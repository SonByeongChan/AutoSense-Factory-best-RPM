# ============================================================================================================
# ëª¨ë¸ ìž¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ìˆ˜ì •ë³¸, ìƒì„¸ ì£¼ì„ í¬í•¨)
#
# ê¸°ëŠ¥ ê°œìš”:
# 1. ì›ë³¸ ê³µì • ë°ì´í„°ì™€ ìƒˆë¡œ ìˆ˜ì§‘ëœ ìš´ì˜ ë°ì´í„°ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
# 2. ì‚¬ìš©ìž ìž…ë ¥ì— ë”°ë¼ íŠ¹ì • ê¸°ê°„(1ê°œì›”, 3ê°œì›” ë“±)ì˜ ë°ì´í„°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
# 3. í´ë¦¬ë‹ì„ í†µí•´ ì´ìƒê°’, ê²°ì¸¡ê°’ ì œê±° í›„ í•™ìŠµì— ì í•©í•œ ë°ì´í„°ì…‹ ìƒì„±í•©ë‹ˆë‹¤.
# 4. ê¸°ì¤€ ë‚ ì§œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ CSVë¡œ ì €ìž¥í•©ë‹ˆë‹¤.
# 5. ì„ íƒì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(--tune ì˜µì…˜ ì œê³µ)
# 6. ëª¨ë¸ í•™ìŠµ í›„ ì„±ëŠ¥ ë¡œê·¸ ë° ê²°ê³¼ ëª¨ë¸ì„ ì €ìž¥í•©ë‹ˆë‹¤.
# ============================================================================================================

# í•„ìš”í•œ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ìž„í¬íŠ¸
import pandas as pd
import numpy as np
import os
import yaml
import joblib
from datetime import datetime
from pandas.tseries.offsets import DateOffset
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
import argparse
import traceback
from utils.model_utils import append_model_log

# ----------------------------------------
# ðŸ“‚ ë””ë ‰í† ë¦¬ ë° ê²½ë¡œ ì„¤ì •
# ----------------------------------------
os.makedirs("config", exist_ok=True)  # ì„¤ì •íŒŒì¼ ì €ìž¥ í´ë”
os.makedirs("models", exist_ok=True)  # ëª¨ë¸ íŒŒì¼ ì €ìž¥ í´ë”
os.makedirs("data", exist_ok=True)    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ìž¥ í´ë”

# íŒŒì¼ ê²½ë¡œ ì •ì˜
RAW_DATA_PATH = "./static/data/imputed_filtered_train_data_2025.csv"
NEW_LOG_PATH = "./static/data/second_1016.csv"
BEST_PARAMS_CONFIG_PATH = "config/best_params.yaml"
ACTIVE_MODEL_CONFIG_PATH = "config/active_model.yaml"
MODELS_DIR = "models"
SCALER_PATH = os.path.join(MODELS_DIR, "Khold_scaler.pkl")
FEATURES_PATH = os.path.join(MODELS_DIR, "Khold_feature_names.pkl")

# ----------------------------------------
# ðŸ› ï¸ ì›ë³¸ + ì‹ ê·œ ë°ì´í„° í†µí•© í•¨ìˆ˜
# ----------------------------------------
def prepare_raw_data():
    """
    ì›ë³¸ ë°ì´í„°ì™€ ì‹ ê·œ ë¡œê·¸ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³ , ì‹œê°„ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    """
    print("="*50)
    print("1. Raw ë°ì´í„° í†µí•© ì‹œìž‘...")

    if not os.path.exists(RAW_DATA_PATH):
        raise FileNotFoundError(f"ì›ë³¸ Raw ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RAW_DATA_PATH}")
    base_df = pd.read_csv(RAW_DATA_PATH, encoding="utf-8-sig")
    print(f"  - ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(base_df)}ê±´")

    if os.path.exists(NEW_LOG_PATH):
        new_df = pd.read_csv(NEW_LOG_PATH, encoding="utf-8-sig")
        print(f"  - ì‹ ê·œ ë°ì´í„° ë¡œë“œ: {len(new_df)}ê±´")
        combined_df = pd.concat([base_df, new_df], ignore_index=True).drop_duplicates(subset=['time'])
    else:
        print("  - ì‹ ê·œ ë°ì´í„° ì—†ìŒ. ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©")
        combined_df = base_df

    print(f"  - í†µí•©ëœ ì´ ë°ì´í„°: {len(combined_df)}ê±´")
    print("="*50)
    return combined_df

# ----------------------------------------
# ðŸ§¼ ë°ì´í„° ì „ì²˜ë¦¬ ë° í•„í„°ë§ í•¨ìˆ˜
# ----------------------------------------
def preprocess_data(df, period, target_date_str):
    """
    í•„í„°ë§ ë° í´ë¦¬ë‹ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ CSVë¡œ ì €ìž¥
    """
    print("2. ë°ì´í„° ì „ì²˜ë¦¬ ì‹œìž‘...")

    df['time'] = pd.to_datetime(df['time'], errors='coerce', utc=True)
    df.dropna(subset=['time'], inplace=True)

    if period != 'all':
        period_map = {'1m': 1, '3m': 3, '6m': 6, '1y': 12}
        months_to_subtract = period_map.get(period)
        end_date = pd.to_datetime(target_date_str, format='%Y%m%d').tz_localize('UTC')
        start_date = end_date - DateOffset(months=months_to_subtract)

        print(f"  - ê¸°ê°„ í•„í„°ë§ ì ìš©: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        df = df[(df['time'] >= start_date) & (df['time'] <= end_date)].copy()
        print(f"  - í•„í„°ë§ í›„ ë°ì´í„° ìˆ˜: {len(df)}ê±´")
    else:
        print("  - ì „ì²´ ê¸°ê°„ ì‚¬ìš©")

    df.loc[df["n_temp_sv"] == 0, "n_temp_sv"] = 70
    invalid_mask = (
        (df["n_temp_sv"] == 0) | (df["k_rpm_pv"] == 0) | (df["E_scr_pv"] == 0) |
        (df["k_rpm_pv"] <= 100) | (df["k_rpm_sv"] <= 100)
    )
    df_clean = df[~invalid_mask].copy()
    df_final = df_clean[(df_clean["scale_pv"] >= 2.90) & (df_clean["scale_pv"] <= 3.30)].copy()

    print(f"  - ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {len(df_final)} (ì œê±°ëœ ìƒ˜í”Œ ìˆ˜: {len(df) - len(df_final)})")

    output_filename = f"{target_date_str}_combined_preprocessed_data.csv"
    output_path = os.path.join("data", output_filename)

    try:
        df_final.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"  - CSV ì €ìž¥ ì™„ë£Œ: {output_path}")
    except Exception as e:
        print(f"  - âŒ ì €ìž¥ ì‹¤íŒ¨: {e}")

    print("="*50)
    return df_final

# ----------------------------------------
# ðŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•¨ìˆ˜
# ----------------------------------------
def tune_hyperparameters_and_save(df):
    """
    GridSearchCVë¥¼ í™œìš©í•œ ëžœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë° ì €ìž¥
    """
    print("3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìž‘...")
    drop_cols = ["time", "scale_pv", "k_rpm_sv", "s_temp_sv", "E_scr_sv", "E_scr_pv", "n_temp_sv", "c_temp_sv", "is_imputed"]
    final_features = df.drop(columns=drop_cols, errors='ignore').columns
    X = df[final_features].astype(float)
    y = df["scale_pv"].astype(float)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    param_grid = {
        "n_estimators": [100, 200, 300],
        "max_depth": [None, 10, 20],
        "min_samples_split": [2, 5, 10],
        "min_samples_leaf": [1, 2, 4],
        "max_features": ["sqrt", "log2"]
    }

    model = RandomForestRegressor(random_state=42, n_jobs=-1)
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    grid_search = GridSearchCV(model, param_grid, scoring="r2", cv=cv, n_jobs=-1, verbose=1)
    grid_search.fit(X_scaled, y)

    best_params = grid_search.best_params_
    print("  - ìµœì  íŒŒë¼ë¯¸í„°:", best_params)
    with open(BEST_PARAMS_CONFIG_PATH, "w") as f:
        yaml.dump(best_params, f, allow_unicode=True)
    print(f"  - ì €ìž¥ ì™„ë£Œ: {BEST_PARAMS_CONFIG_PATH}")
    print("="*50)

# ----------------------------------------
# ðŸ¤– ëª¨ë¸ í•™ìŠµ ë° ì €ìž¥ í•¨ìˆ˜
# ----------------------------------------
def train_and_save_model(df, target_date_str):
    """
    RandomForestRegressor í•™ìŠµ í›„ ëª¨ë¸ ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ìž¥
    """
    print("4. ëª¨ë¸ í•™ìŠµ ë° ì €ìž¥ ì‹œìž‘...")
    drop_cols = ["time", "scale_pv", "k_rpm_sv", "s_temp_sv", "E_scr_sv", "E_scr_pv", "n_temp_sv", "c_temp_sv", "is_imputed"]
    final_features = df.drop(columns=drop_cols, errors='ignore').columns
    X = df[final_features].astype(float)
    y = df["scale_pv"].astype(float)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, SCALER_PATH)
    joblib.dump(final_features.tolist(), FEATURES_PATH)

    print("  - ìŠ¤ì¼€ì¼ëŸ¬ ë° í”¼ì²˜ ì €ìž¥ ì™„ë£Œ")

    if not os.path.exists(BEST_PARAMS_CONFIG_PATH):
        raise FileNotFoundError("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. --tune ì˜µì…˜ìœ¼ë¡œ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    with open(BEST_PARAMS_CONFIG_PATH, "r") as f:
        best_params = yaml.safe_load(f)

    model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)
    model.fit(X_scaled, y)

    y_pred = model.predict(X_scaled)
    r2 = model.score(X_scaled, y)
    mse = mean_squared_error(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mse)

    print(f"  - RÂ²: {r2:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE : {mae:.4f}")

    filename = f"{target_date_str}_RF_r2_{r2:.4f}_mae_{mae:.4f}.pkl"
    filepath = os.path.join(MODELS_DIR, filename)
    joblib.dump(model, filepath)
    print(f"  - ëª¨ë¸ ì €ìž¥ ì™„ë£Œ: {filepath}")

    log_path = os.path.join(MODELS_DIR, "model_performance_log.csv")
    log_entry = pd.DataFrame([{
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "model_filename": filename,
        "r2": r2,
        "mse": mse,
        "rmse": rmse,
        "data_size": len(df)
    }])
    if not os.path.exists(log_path):
        log_entry.to_csv(log_path, index=False, encoding='utf-8-sig')
    else:
        log_entry.to_csv(log_path, mode='a', header=False, index=False, encoding='utf-8-sig')

    with open(ACTIVE_MODEL_CONFIG_PATH, "w") as f:
        yaml.dump({
            "model_path": filepath,
            "scaler_path": SCALER_PATH,
            "feature_names_path": FEATURES_PATH,
            "performance": {"r2": r2, "mse": mse, "rmse": rmse}
        }, f, allow_unicode=True, sort_keys=False)


    append_model_log(
    date=datetime.now().strftime('%Y-%m-%d %H:%M'),
    mae=mae,
    loss=np.sum(np.abs(y_pred - 3.00)),
    loss_rate=np.sum(np.abs(y_pred - 3.00)) / np.sum(y),
    model_name=filename.replace('.pkl', '')
)


print("="*50)



# ----------------------------------------
# ðŸš€ ë©”ì¸ í•¨ìˆ˜: ì¸ìž íŒŒì‹± ë° ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ----------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ìž¬í•™ìŠµ íŒŒì´í”„ë¼ì¸")
    parser.add_argument('--tune', action='store_true', help="í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰ ì—¬ë¶€")
    parser.add_argument('--period', type=str, default='all', choices=['1m', '3m', '6m', '1y', 'all'], help="ìž¬í•™ìŠµ ê¸°ê°„ ì„ íƒ")
    parser.add_argument('--date', type=str, default=None, help="ê¸°ì¤€ ë‚ ì§œ ì§€ì • (YYYYMMDD)")
    args = parser.parse_args()

    # ë‚ ì§œ ê²€ì¦ ë° ì„¤ì •

    if args.date:
        try:
            datetime.strptime(args.date, '%Y%m%d')
            target_date_str = args.date
        except ValueError:
            print("âŒ ìž˜ëª»ëœ ë‚ ì§œ í˜•ì‹ìž…ë‹ˆë‹¤. YYYYMMDD í˜•ì‹ìœ¼ë¡œ ìž…ë ¥í•´ì£¼ì„¸ìš”.")
            exit()
    else:
        target_date_str = datetime.now().strftime('%Y%m%d')

    print(f"ðŸš€ ê¸°ì¤€ ë‚ ì§œ: {target_date_str} / ë°ì´í„° ë²”ìœ„: {args.period}")

    try:
        raw_combined_data = prepare_raw_data()
        clean_data = preprocess_data(raw_combined_data, args.period, target_date_str)

        if clean_data.empty:
            print("âŒ ì „ì²˜ë¦¬ í›„ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if args.tune:
                tune_hyperparameters_and_save(clean_data)
            else:
                print("3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìƒëžµ (ê¸°ì¡´ ê°’ ì‚¬ìš©)")
            train_and_save_model(clean_data, target_date_str)
            print("ðŸŽ‰ ëª¨ë¸ ìž¬í•™ìŠµ ì™„ë£Œ")

    except Exception as e:
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)
        traceback.print_exc()
